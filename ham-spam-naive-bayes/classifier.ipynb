{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Preprocessing\n",
    "Tokenize each line in the dataset into words, ignore punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Convert sentences to words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Removing unnecessary punctuation and tags\n",
    "    words = [re.sub(r\"[^a-zA-Z0-9]\", \"\", word) for word in words]\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(['the', 'is', 'are', 'and', 'that', 'do', 'have'])\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming (using a simple stemming rule)\n",
    "    words = [word[:-1] if word.endswith('s') else word for word in words]\n",
    "    \n",
    "    # Lemmatization (not implemented in this example)\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def tokenize_csv(file_path, type, cutt_off):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            if type == 'train' and i >= cutt_off:\n",
    "                continue\n",
    "\n",
    "            if type == 'test' and i < cutt_off:\n",
    "                continue\n",
    "            \n",
    "            tokenized_row = {}\n",
    "            \n",
    "            # Tokenize the 'text' column\n",
    "            tokenized_text = tokenize_text(row['text'])\n",
    "            tokenized_row['text'] = tokenized_text\n",
    "            \n",
    "            # Preserve the 'type' column as is\n",
    "            tokenized_row['type'] = row['type']\n",
    "            \n",
    "            tokenized_data.append(tokenized_row)\n",
    "    \n",
    "    return tokenized_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model and Training\n",
    "Split the data (consisting of 5559 messages) and train the model on the 5000 of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutt_off = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "training_data = tokenize_csv(file_path, 'train', cutt_off)\n",
    "\n",
    "\n",
    "spam, ham = [], []\n",
    "total_spam_messages, total_ham_messages = 0, 0\n",
    "\n",
    "for message in training_data:\n",
    "    text, type = message['text'], message['type']\n",
    "\n",
    "    if type == 'spam':\n",
    "        spam.extend(text)\n",
    "        total_spam_messages += 1\n",
    "    else:\n",
    "        ham.extend(text)\n",
    "        total_ham_messages += 1\n",
    "\n",
    "total_words_in_ham = len(ham)\n",
    "total_words_in_spam = len(spam)\n",
    "\n",
    "count_ham = Counter(ham)\n",
    "count_spam = Counter(spam)\n",
    "\n",
    "likelyhood_ham = {word: count_ham[word] / total_words_in_ham for word in count_ham}\n",
    "likelyhood_spam = {word: count_spam[word] / total_words_in_spam for word in count_spam}\n",
    "\n",
    "probability_ham = total_ham_messages / (total_ham_messages + total_spam_messages)\n",
    "probability_spam = total_spam_messages / (total_ham_messages + total_spam_messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.I. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message):\n",
    "    probability_ham_message_given_message = 1\n",
    "    words_given_ham = 1\n",
    "    words_given_spam = 1\n",
    "\n",
    "    for word in message:\n",
    "        words_given_ham *= likelyhood_ham[word] if word in likelyhood_ham else 1 / total_words_in_ham\n",
    "        words_given_spam *= likelyhood_spam[word] if word in likelyhood_spam else 1 / total_words_in_spam\n",
    "    \n",
    "    probability_ham_message_given_message *= words_given_ham * probability_ham\n",
    "    probability_spam_message_given_message = words_given_spam * probability_spam\n",
    "\n",
    "    if probability_ham_message_given_message > probability_spam_message_given_message:\n",
    "        return 'ham'\n",
    "    return 'spam'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.II. Moment of Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9481216457960644\n"
     ]
    }
   ],
   "source": [
    "testing_data = tokenize_csv(file_path, 'test', cutt_off)\n",
    "\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "\n",
    "for message in testing_data:\n",
    "    if predict(message['text']) == message['type']:\n",
    "        count_correct += 1\n",
    "    else:\n",
    "        count_incorrect += 1\n",
    "\n",
    "print(\"Accuracy:\", count_correct / (count_correct + count_incorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
